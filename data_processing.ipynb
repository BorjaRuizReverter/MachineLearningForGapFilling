{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data\n",
    "\n",
    "## Step 1: Upload the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Rg        rH\n",
      "1      467.124900  59.81609\n",
      "2      567.265600  54.25453\n",
      "3      510.038200  50.37691\n",
      "4      562.113500  47.96687\n",
      "5      489.934200  45.71235\n",
      "...           ...       ...\n",
      "11360   -2.632179  87.36536\n",
      "11361   -0.885231  88.51657\n",
      "11362   -1.904163  87.13349\n",
      "11363   -1.932571  87.48014\n",
      "11364   -1.310510  87.57803\n",
      "\n",
      "[11361 rows x 2 columns]\n",
      "            NEE\n",
      "1     -1.282251\n",
      "2     -5.457097\n",
      "3     -3.174246\n",
      "4     -3.362150\n",
      "5     -3.737496\n",
      "...         ...\n",
      "11360  2.652203\n",
      "11361  0.893950\n",
      "11362  1.672291\n",
      "11363  0.844095\n",
      "11364  0.832299\n",
      "\n",
      "[11361 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_pickle('X.pkl')\n",
    "y = pd.read_pickle('y.pkl')\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Rg        rH\n",
      "3982   237.787300  40.95904\n",
      "589    611.959500  55.40580\n",
      "7695    -2.076583  82.20747\n",
      "2158    -2.639731  94.10475\n",
      "6620    -3.033176  77.12545\n",
      "...           ...       ...\n",
      "7817    -2.844407  86.26534\n",
      "10959   -0.778399  88.79167\n",
      "906     -1.962875  93.33716\n",
      "5196     2.517354  79.44413\n",
      "236    502.330800  63.65034\n",
      "\n",
      "[8520 rows x 2 columns]\n",
      "17040\n",
      "              Rg         rH\n",
      "9668   89.029820   86.29816\n",
      "9207  780.027400   36.52398\n",
      "8007   -0.977286   98.51484\n",
      "2249   -2.867806   90.99128\n",
      "9200  343.705000   59.81927\n",
      "...          ...        ...\n",
      "3194   -0.983599   87.66892\n",
      "9891    6.345633   68.90066\n",
      "6479   -0.931993   97.25005\n",
      "7946   -1.456769   85.97623\n",
      "5007   -1.289494  100.03700\n",
      "\n",
      "[2841 rows x 2 columns]\n",
      "5682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "Data is splitted into train and test\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train)\n",
    "print(X_train.size)\n",
    "print(X_test)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NEE\n",
      "3982   1.478472\n",
      "589   -4.249636\n",
      "7695   3.894052\n",
      "2158   3.945240\n",
      "6620   4.294096\n",
      "...         ...\n",
      "7817   1.878224\n",
      "10959  0.973065\n",
      "906    3.682107\n",
      "5196   2.751083\n",
      "236   -1.585572\n",
      "\n",
      "[8520 rows x 1 columns]\n",
      "8520\n",
      "           NEE\n",
      "9668 -1.346135\n",
      "9207 -6.474561\n",
      "8007  4.732932\n",
      "2249  4.329803\n",
      "9200 -4.570991\n",
      "...        ...\n",
      "3194  0.641850\n",
      "9891  2.095709\n",
      "6479  1.144712\n",
      "7946  2.675923\n",
      "5007  0.973291\n",
      "\n",
      "[2841 rows x 1 columns]\n",
      "2841\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.size)\n",
    "print(y_test)\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the model\n",
    "The goal is to predict future behavor of a time serie, so I apply a regression model.\n",
    "\n",
    "Below I initialize the model into the object 'regreModel'. Then I apply the method 'fit' to it. This method will use the data to train the model and build it.\n",
    "\n",
    "The model used is the Multi-layer Perceptron Regressor (MLP Regressor). This is an Artificial Neural Network composed of different layers with several neurons each.\n",
    "\n",
    "These and more parameters can be selected in the function. For the whole description, visit the scikit website for this model (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#examples-using-sklearn-neural-network-mlpregressor)\n",
    "\n",
    "I only set the following config different from default:\n",
    "1. random_state to 1, which means that 1 will be initial state of the weights of every neuron.\n",
    "2. max_iter to 400, in order to prevent the model from unconvergence.\n",
    "3. verbose to True, in order to print progress messages to stdout.\n",
    "\n",
    "One of the most importart parameters, by the way, has to do with the variants of Gradient Descent algorithm. This is the so called solver. This model has the following possibilities:\n",
    "1. LBFGS, or Limited-memory BFGS is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm using a limited amount of computer memory.\n",
    "2. SGD, or Stochastic Gradient Descent, where only one training example is used to compute the gradient and update the parameters at each iteration. This can be faster than batch gradient descent but may lead to more noise in the updates.\n",
    "3. ADAM (default), where adam stands for Adaptive Moment estimation. The adam algorithm combines the benefits of Momentum-based Gradient Descent, Adagrad, and RMSprop.\n",
    "\n",
    "The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borja/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 414.97882900\n",
      "Iteration 2, loss = 10.38681335\n",
      "Iteration 3, loss = 4.82978731\n",
      "Iteration 4, loss = 4.13940614\n",
      "Iteration 5, loss = 3.99579314\n",
      "Iteration 6, loss = 3.95695062\n",
      "Iteration 7, loss = 3.95257378\n",
      "Iteration 8, loss = 3.94276638\n",
      "Iteration 9, loss = 3.94048259\n",
      "Iteration 10, loss = 3.93699180\n",
      "Iteration 11, loss = 3.94915482\n",
      "Iteration 12, loss = 3.95000307\n",
      "Iteration 13, loss = 3.93166300\n",
      "Iteration 14, loss = 3.93669877\n",
      "Iteration 15, loss = 3.93226425\n",
      "Iteration 16, loss = 3.92490388\n",
      "Iteration 17, loss = 3.92684670\n",
      "Iteration 18, loss = 3.92564482\n",
      "Iteration 19, loss = 3.93750215\n",
      "Iteration 20, loss = 3.94523789\n",
      "Iteration 21, loss = 3.94423637\n",
      "Iteration 22, loss = 3.91794321\n",
      "Iteration 23, loss = 3.93371484\n",
      "Iteration 24, loss = 3.94832099\n",
      "Iteration 25, loss = 3.92464954\n",
      "Iteration 26, loss = 3.92406757\n",
      "Iteration 27, loss = 3.94986513\n",
      "Iteration 28, loss = 3.92275847\n",
      "Iteration 29, loss = 3.90784902\n",
      "Iteration 30, loss = 3.90828279\n",
      "Iteration 31, loss = 3.94528057\n",
      "Iteration 32, loss = 3.93333368\n",
      "Iteration 33, loss = 3.92950989\n",
      "Iteration 34, loss = 3.91686602\n",
      "Iteration 35, loss = 3.94024873\n",
      "Iteration 36, loss = 3.92895843\n",
      "Iteration 37, loss = 3.91154322\n",
      "Iteration 38, loss = 3.91141240\n",
      "Iteration 39, loss = 3.92748178\n",
      "Iteration 40, loss = 3.90971194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(max_iter=400, random_state=1, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(max_iter=400, random_state=1, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(max_iter=400, random_state=1, verbose=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "regreModel = MLPRegressor(random_state = 1, max_iter = 400, verbose = True)\n",
    "regreModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Checking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5254528228752913"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regreModel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16271392, -5.71196625])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regreModel.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regreModel.model']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I cannot do the following.\n",
    "It seems that the sklearn.externals package is deprecated in recent versions of scikit-learn\n",
    "'''\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(regreModel, 'regreModel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
